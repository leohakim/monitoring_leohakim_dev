# GuÃ­a de Monitoreo - Elecciones 2025

## ðŸ“Š Stack de Monitoreo

Sistema completo de observabilidad para monitorear **Production** y **Standby** con mÃ©tricas detalladas, logs centralizados y dashboards para anÃ¡lisis post-evento.

### Componentes

| Componente | PropÃ³sito | Puerto |
|------------|-----------|--------|
| **Prometheus** | RecolecciÃ³n y almacenamiento de mÃ©tricas | 9090 |
| **Grafana** | VisualizaciÃ³n y dashboards | 3000 |
| **Loki** | AgregaciÃ³n de logs | 3100 |
| **AlertManager** | GestiÃ³n de alertas | 9093 |
| **cAdvisor** | MÃ©tricas de contenedores | 8080 |
| **Node Exporter** | MÃ©tricas del SO | 9100 |
| **PostgreSQL Exporter** | MÃ©tricas de BD | 9187 |
| **Redis Exporter** | MÃ©tricas de Redis | 9121 |
| **Promtail** | Recolector de logs | 9080 |

---

## ðŸŽ¯ MÃ©tricas Recolectadas

### **Velocidad/Performance**
- âœ… Latencia HTTP (p50, p95, p99)
- âœ… Throughput (requests/segundo)
- âœ… Tiempo de respuesta de BD
- âœ… Cache hit ratio de Redis
- âœ… Tiempo de procesamiento Celery

### **Resiliencia**
- âœ… Tasa de errores (4xx, 5xx)
- âœ… Reintentos de requests
- âœ… Reintentos de tareas Celery
- âœ… Conexiones fallidas
- âœ… Health checks fallidos

### **Disponibilidad**
- âœ… Uptime por servicio
- âœ… SLA del sistema
- âœ… Tiempo de recuperaciÃ³n
- âœ… Disponibilidad de endpoints

### **Recursos**
- âœ… CPU (por servicio y total)
- âœ… RAM (por servicio y total)
- âœ… Disco I/O y espacio
- âœ… Red (bandwidth)
- âœ… Conexiones activas

### **AplicaciÃ³n**
- âœ… Usuarios concurrentes
- âœ… Sesiones activas
- âœ… Queries a la API
- âœ… Tareas Celery (pending/active/completed/failed)
- âœ… Cache hits/misses

---

## ðŸš€ Paso 1: Despliegue del Servidor de Monitoreo

### OpciÃ³n A: Servidor Dedicado (Recomendado)

**Requisitos mÃ­nimos:**
- 2 CPU cores
- 4GB RAM
- 50GB disco (para retenciÃ³n de 90 dÃ­as)
- Debian 12 / Ubuntu 22.04

```bash
# En el servidor de monitoreo
cd /opt/apps
git clone <REPO> elecciones-monitoring
cd elecciones-monitoring

# Configurar variables de entorno
nano .env
```

**Contenido de `.env`:**
```bash
GRAFANA_ADMIN_PASSWORD=TU_PASSWORD_SEGURO
```

```bash
# Iniciar servicios de monitoreo
docker compose -f docker-compose.monitoring.yml up -d

# Verificar que todo estÃ¡ corriendo
docker compose -f docker-compose.monitoring.yml ps
```

### OpciÃ³n B: En el Mismo Servidor de Production

Si no tienes un servidor dedicado, puedes desplegar en production (requiere mÃ¡s recursos):

```bash
cd /opt/server/elecciones-nacionales-octubre-2025
docker compose -f docker-compose.production.yml -f docker-compose.monitoring.yml up -d
```

---

## ðŸ”§ Paso 2: Configurar IPs de los Servidores

### 2.1. Editar Prometheus

```bash
nano compose/monitoring/prometheus/prometheus.yml
```

**Buscar y reemplazar:**
- `IP_PRODUCTION` â†’ IP real del servidor production (ej: `192.168.1.10`)
- `IP_STANDBY` â†’ IP real del servidor standby (ej: `51.38.233.100`)

**Ejemplo:**
```yaml
- targets: ['192.168.1.10:8080']  # cAdvisor production
- targets: ['51.38.233.100:8080']  # cAdvisor standby
```

### 2.2. Recargar configuraciÃ³n de Prometheus

```bash
# Recargar sin reiniciar (hot reload)
curl -X POST http://localhost:9090/-/reload

# O reiniciar el contenedor
docker compose -f docker-compose.monitoring.yml restart prometheus
```

---

## ðŸ“¡ Paso 3: Desplegar Exporters en Production

### 3.1. En el servidor de Production

```bash
cd /opt/server/elecciones-nacionales-octubre-2025

# Configurar IP del servidor de monitoreo
nano docker-compose.exporters.yml
```

**Buscar y cambiar:**
```yaml
LOKI_URL: "http://IP_MONITORING_SERVER:3100"
```

**Ejemplo:**
```yaml
LOKI_URL: "http://192.168.1.5:3100"  # IP del servidor de monitoreo
```

### 3.2. Configurar variable de entorno para PostgreSQL

```bash
nano .envs/.production/.postgres
```

**Agregar (si no existe):**
```bash
POSTGRES_PASSWORD=tu_password_actual
```

### 3.3. Iniciar exporters

```bash
# Iniciar production + exporters
docker compose -f docker-compose.production.yml -f docker-compose.exporters.yml up -d

# Verificar que los exporters estÃ¡n corriendo
docker compose -f docker-compose.production.yml -f docker-compose.exporters.yml ps | grep exporter
```

### 3.4. Abrir puertos en firewall

```bash
# Permitir acceso desde el servidor de monitoreo
ufw allow from IP_MONITORING_SERVER to any port 8080  # cAdvisor
ufw allow from IP_MONITORING_SERVER to any port 9100  # Node Exporter
ufw allow from IP_MONITORING_SERVER to any port 9187  # PostgreSQL Exporter
ufw allow from IP_MONITORING_SERVER to any port 9121  # Redis Exporter
```

---

## ðŸ“¡ Paso 4: Desplegar Exporters en Standby

### 4.1. En el servidor de Standby

```bash
cd /opt/server/elecciones-nacionales-octubre-2025

# Configurar IP del servidor de monitoreo
nano docker-compose.exporters.yml
```

**Cambiar:**
```yaml
LOKI_URL: "http://IP_MONITORING_SERVER:3100"
```

### 4.2. Configurar variable de entorno para PostgreSQL

```bash
nano .envs/.standby/.postgres
```

**Verificar que existe:**
```bash
POSTGRES_PASSWORD=tu_password_standby
```

### 4.3. Iniciar exporters

```bash
# Iniciar standby + exporters
docker compose -f docker-compose.standby.yml -f docker-compose.exporters.yml up -d

# Verificar
docker compose -f docker-compose.standby.yml -f docker-compose.exporters.yml ps | grep exporter
```

### 4.4. Abrir puertos en firewall

```bash
ufw allow from IP_MONITORING_SERVER to any port 8080
ufw allow from IP_MONITORING_SERVER to any port 9100
ufw allow from IP_MONITORING_SERVER to any port 9187
ufw allow from IP_MONITORING_SERVER to any port 9121
```

---

## âœ… Paso 5: Verificar Conectividad

### 5.1. Desde el servidor de monitoreo

```bash
# Verificar que Prometheus puede alcanzar los exporters

# Production
curl http://IP_PRODUCTION:8080/metrics  # cAdvisor
curl http://IP_PRODUCTION:9100/metrics  # Node Exporter
curl http://IP_PRODUCTION:9187/metrics  # PostgreSQL Exporter
curl http://IP_PRODUCTION:9121/metrics  # Redis Exporter

# Standby
curl http://IP_STANDBY:8080/metrics
curl http://IP_STANDBY:9100/metrics
curl http://IP_STANDBY:9187/metrics
curl http://IP_STANDBY:9121/metrics
```

### 5.2. Verificar targets en Prometheus

```bash
# Abrir en navegador
http://IP_MONITORING_SERVER:9090/targets

# Todos los targets deben estar en estado "UP" (verde)
```

---

## ðŸ“Š Paso 6: Acceder a Grafana

### 6.1. Abrir Grafana

```
URL: http://IP_MONITORING_SERVER:3000
Usuario: admin
Password: (el que configuraste en GRAFANA_ADMIN_PASSWORD)
```

### 6.2. Verificar datasources

1. Ir a **Configuration** â†’ **Data Sources**
2. Verificar que **Prometheus** y **Loki** estÃ¡n conectados (verde)

### 6.3. Importar dashboards adicionales

Grafana tiene dashboards pre-hechos excelentes:

**Dashboards recomendados:**

1. **Node Exporter Full** (ID: 1860)
   - MÃ©tricas completas del sistema operativo
   
2. **Docker Container & Host Metrics** (ID: 179)
   - MÃ©tricas de contenedores Docker
   
3. **PostgreSQL Database** (ID: 9628)
   - MÃ©tricas detalladas de PostgreSQL
   
4. **Redis Dashboard** (ID: 11835)
   - MÃ©tricas de Redis

**Para importar:**
1. Ir a **Dashboards** â†’ **Import**
2. Ingresar el ID del dashboard
3. Seleccionar datasource **Prometheus**
4. Click en **Import**

---

## ðŸ“ˆ Paso 7: Crear Dashboard Personalizado para Stakeholders

### 7.1. Crear nuevo dashboard

1. **Dashboards** â†’ **New Dashboard** â†’ **Add visualization**
2. Seleccionar datasource **Prometheus**

### 7.2. Paneles recomendados para el reporte

#### Panel 1: Uptime Total
```promql
avg(up{job=~".*production.*|.*standby.*"}) * 100
```
**Tipo:** Stat
**TÃ­tulo:** Disponibilidad Total (%)

#### Panel 2: Requests por Segundo
```promql
sum(rate(traefik_service_requests_total[5m])) by (server)
```
**Tipo:** Time series
**TÃ­tulo:** Throughput (req/s)

#### Panel 3: Latencia p95
```promql
histogram_quantile(0.95, rate(traefik_service_request_duration_seconds_bucket[5m]))
```
**Tipo:** Time series
**TÃ­tulo:** Latencia p95 (segundos)

#### Panel 4: Tasa de Errores
```promql
sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) / sum(rate(traefik_service_requests_total[5m])) * 100
```
**Tipo:** Time series
**TÃ­tulo:** Tasa de Errores 5xx (%)

#### Panel 5: CPU Usage por Servidor
```promql
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```
**Tipo:** Time series
**TÃ­tulo:** CPU Usage (%)

#### Panel 6: Memory Usage por Servidor
```promql
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
```
**Tipo:** Time series
**TÃ­tulo:** Memory Usage (%)

#### Panel 7: Conexiones a PostgreSQL
```promql
pg_stat_activity_count
```
**Tipo:** Time series
**TÃ­tulo:** Conexiones Activas a BD

#### Panel 8: Tareas Celery
```promql
sum(celery_tasks_total) by (state, server)
```
**Tipo:** Time series
**TÃ­tulo:** Tareas Celery por Estado

---

## ðŸ“ Paso 8: Configurar Anotaciones para Eventos

Las anotaciones te permiten marcar eventos importantes en los dashboards.

### 8.1. Crear anotaciÃ³n manual

1. En cualquier dashboard, hacer click en un punto del grÃ¡fico
2. **Add annotation**
3. Agregar descripciÃ³n (ej: "Inicio de votaciÃ³n", "Cierre de mesas", "Pico de trÃ¡fico")
4. **Save**

### 8.2. Anotaciones automÃ¡ticas desde Prometheus

Editar dashboard â†’ **Settings** â†’ **Annotations** â†’ **Add annotation query**

```promql
ALERTS{alertstate="firing"}
```

Esto mostrarÃ¡ automÃ¡ticamente cuando se disparan alertas.

---

## ðŸ“Š Paso 9: Generar Reportes Post-Evento

### 9.1. Exportar dashboard como PDF

1. Abrir el dashboard que quieres exportar
2. Click en **Share** (icono de compartir)
3. **Export** â†’ **Save as PDF**
4. Configurar:
   - Time range: Seleccionar el perÃ­odo del evento
   - Layout: Portrait o Landscape
5. **Save as PDF**

### 9.2. Exportar datos crudos

```bash
# Desde el servidor de monitoreo

# Exportar snapshot de Prometheus
docker exec monitoring_prometheus promtool tsdb dump /prometheus > metrics_snapshot.json

# Comprimir
gzip metrics_snapshot.json

# Descargar a tu mÃ¡quina local
scp user@monitoring_server:/path/metrics_snapshot.json.gz ./
```

### 9.3. Queries Ãºtiles para el reporte

#### Disponibilidad Total (SLA)
```promql
avg_over_time(up{job=~".*production.*"}[24h]) * 100
```

#### Throughput Promedio
```promql
avg_over_time(rate(traefik_service_requests_total[5m])[24h])
```

#### Throughput MÃ¡ximo
```promql
max_over_time(rate(traefik_service_requests_total[5m])[24h])
```

#### Latencia Promedio
```promql
avg_over_time(histogram_quantile(0.95, rate(traefik_service_request_duration_seconds_bucket[5m]))[24h])
```

#### Total de Requests
```promql
sum(increase(traefik_service_requests_total[24h]))
```

#### Total de Errores
```promql
sum(increase(traefik_service_requests_total{code=~"5.."}[24h]))
```

#### CPU Promedio
```promql
avg_over_time((100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100))[24h])
```

#### CPU MÃ¡ximo
```promql
max_over_time((100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100))[24h])
```

#### Memoria Promedio
```promql
avg_over_time(((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100)[24h])
```

---

## ðŸ“ˆ Paso 10: AnÃ¡lisis y Storytelling para Stakeholders

### 10.1. Estructura del Reporte

**1. Executive Summary**
- Disponibilidad total (SLA)
- Throughput total
- Usuarios concurrentes pico
- Incidentes crÃ­ticos (si hubo)

**2. Performance**
- GrÃ¡fico de requests/segundo durante el evento
- Latencia promedio y picos
- ComparaciÃ³n production vs standby (si se usÃ³)

**3. Resiliencia**
- Tasa de errores
- Tiempo de respuesta ante incidentes
- Efectividad de alertas

**4. Recursos**
- Uso de CPU/RAM/Disco
- IdentificaciÃ³n de cuellos de botella
- Margen de capacidad restante

**5. Lecciones Aprendidas**
- QuÃ© funcionÃ³ bien
- QuÃ© se puede mejorar
- Recomendaciones para futuros eventos

### 10.2. MÃ©tricas Clave para Stakeholders

| MÃ©trica | DescripciÃ³n | Query Prometheus |
|---------|-------------|------------------|
| **SLA** | % de disponibilidad | `avg_over_time(up[24h]) * 100` |
| **Throughput Pico** | MÃ¡ximo req/s | `max_over_time(rate(requests_total[5m])[24h])` |
| **Usuarios Concurrentes** | MÃ¡ximo simultÃ¡neo | `max_over_time(active_sessions[24h])` |
| **Latencia p95** | 95% de requests < X ms | `histogram_quantile(0.95, ...)` |
| **Tasa de Errores** | % de requests fallidos | `sum(errors) / sum(total) * 100` |
| **Tiempo de RecuperaciÃ³n** | MTTR de incidentes | Manual desde anotaciones |

### 10.3. Visualizaciones Recomendadas

1. **LÃ­nea de tiempo del evento**
   - Requests/segundo con anotaciones de eventos clave
   
2. **Heatmap de latencia**
   - Mostrar distribuciÃ³n de latencias a lo largo del tiempo
   
3. **ComparaciÃ³n Production vs Standby**
   - Si se activÃ³ standby, mostrar transiciÃ³n
   
4. **Uso de recursos**
   - GrÃ¡ficos de Ã¡rea apilada de CPU/RAM
   
5. **Top errores**
   - Tabla con los errores mÃ¡s frecuentes

---

## ðŸ”” ConfiguraciÃ³n de Alertas (Opcional)

### Alertas ya configuradas

El sistema incluye alertas para:
- âœ… Servicios caÃ­dos
- âœ… Alto uso de CPU (>80%)
- âœ… Alto uso de memoria (>85%)
- âœ… Alto uso de disco (>80%)
- âœ… PostgreSQL caÃ­do
- âœ… Redis caÃ­do
- âœ… Queries lentas
- âœ… Alta tasa de errores

### Configurar notificaciones

#### OpciÃ³n 1: Email (requiere SMTP)

```bash
nano compose/monitoring/prometheus/alertmanager.yml
```

```yaml
receivers:
  - name: 'critical'
    email_configs:
      - to: 'tu-email@ejemplo.com'
        from: 'alertas@elecciones.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'tu-email@gmail.com'
        auth_password: 'tu-app-password'
```

#### OpciÃ³n 2: Webhook (Slack, Discord, etc.)

```yaml
receivers:
  - name: 'critical'
    webhook_configs:
      - url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        send_resolved: true
```

#### OpciÃ³n 3: Telegram

```yaml
receivers:
  - name: 'critical'
    telegram_configs:
      - bot_token: 'YOUR_BOT_TOKEN'
        chat_id: YOUR_CHAT_ID
```

---

## ðŸ› ï¸ Mantenimiento y Troubleshooting

### Ver logs de servicios de monitoreo

```bash
# Prometheus
docker compose -f docker-compose.monitoring.yml logs -f prometheus

# Grafana
docker compose -f docker-compose.monitoring.yml logs -f grafana

# Loki
docker compose -f docker-compose.monitoring.yml logs -f loki
```

### Verificar espacio en disco

```bash
# Ver uso de volÃºmenes
docker system df -v

# Limpiar datos antiguos (cuidado!)
# Esto eliminarÃ¡ mÃ©tricas mÃ¡s allÃ¡ de la retenciÃ³n configurada
docker exec monitoring_prometheus promtool tsdb cleanup /prometheus
```

### Backup de dashboards de Grafana

```bash
# Exportar todos los dashboards
docker exec monitoring_grafana grafana-cli admin export-dashboard > dashboards_backup.json

# O copiar el volumen completo
docker run --rm -v monitoring_grafana_data:/data -v $(pwd):/backup alpine tar czf /backup/grafana_backup.tar.gz /data
```

### Restaurar desde backup

```bash
# Restaurar volumen de Grafana
docker run --rm -v monitoring_grafana_data:/data -v $(pwd):/backup alpine tar xzf /backup/grafana_backup.tar.gz -C /
```

### Problema: Targets en estado "DOWN"

```bash
# 1. Verificar conectividad de red
ping IP_TARGET

# 2. Verificar que el exporter estÃ¡ corriendo
curl http://IP_TARGET:PORT/metrics

# 3. Verificar firewall
ufw status

# 4. Ver logs del exporter
docker logs monitoring_node_exporter
```

### Problema: Grafana no muestra datos

```bash
# 1. Verificar datasource
curl http://localhost:9090/api/v1/query?query=up

# 2. Verificar que Prometheus tiene datos
docker exec monitoring_prometheus promtool tsdb list /prometheus

# 3. Reiniciar Grafana
docker compose -f docker-compose.monitoring.yml restart grafana
```

---

## ðŸ“š Recursos Adicionales

### DocumentaciÃ³n Oficial
- [Prometheus](https://prometheus.io/docs/)
- [Grafana](https://grafana.com/docs/)
- [Loki](https://grafana.com/docs/loki/)

### Dashboards de la Comunidad
- [Grafana Dashboards](https://grafana.com/grafana/dashboards/)

### PromQL (Lenguaje de queries)
- [PromQL Basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [PromQL Examples](https://prometheus.io/docs/prometheus/latest/querying/examples/)

---

## ðŸ“ž Checklist de Despliegue

- [ ] Servidor de monitoreo desplegado
- [ ] IPs configuradas en prometheus.yml
- [ ] Exporters desplegados en production
- [ ] Exporters desplegados en standby
- [ ] Puertos de firewall abiertos
- [ ] Todos los targets en estado "UP"
- [ ] Grafana accesible
- [ ] Datasources conectados
- [ ] Dashboards importados
- [ ] Dashboard personalizado creado
- [ ] Alertas configuradas (opcional)
- [ ] Notificaciones configuradas (opcional)
- [ ] Backup de configuraciÃ³n realizado

---

## ðŸŽ¯ Resumen de Puertos

| Servicio | Puerto | Acceso |
|----------|--------|--------|
| Prometheus | 9090 | Interno |
| Grafana | 3000 | PÃºblico (con auth) |
| Loki | 3100 | Interno |
| AlertManager | 9093 | Interno |
| cAdvisor | 8080 | Interno (desde monitoring) |
| Node Exporter | 9100 | Interno (desde monitoring) |
| PostgreSQL Exporter | 9187 | Interno (desde monitoring) |
| Redis Exporter | 9121 | Interno (desde monitoring) |

---

**Ãšltima actualizaciÃ³n**: Octubre 2025  
**VersiÃ³n**: 1.0  
**RetenciÃ³n de datos**: 90 dÃ­as  
**Contacto**: leohakim@gmail.com
